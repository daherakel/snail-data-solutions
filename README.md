# Snail Data Solutions

Repositorio de soluciones de Data Engineering y AI, incluyendo pipelines con Apache Airflow y dbt, y agentes de AI con AWS Bedrock.

## ðŸ› ï¸ Stack TecnolÃ³gico

### OrquestaciÃ³n y TransformaciÃ³n
- **Airflow**: 2.10.3 (Astro Runtime 12.5.0)
- **dbt**: 1.10.15 con adaptador PostgreSQL 1.9.1
- **Astronomer CLI**: Para desarrollo local
- **PostgreSQL**: 13 (para ejemplos locales)

### Cloud e AI
- **AWS Bedrock**: Agentes de AI con Claude/Titan
- **AWS Lambda**: Procesamiento serverless
- **AWS Step Functions**: OrquestaciÃ³n de workflows
- **Amazon S3**: Storage de documentos
- **Amazon Textract**: OCR para imÃ¡genes

### Infraestructura
- **Terraform**: Infrastructure as Code
- **Docker**: ContainerizaciÃ³n
- **Git**: Control de versiones

## ðŸ“ Estructura del Proyecto

```
snail-data-solutions/
â”œâ”€â”€ modules/                                # Todos los mÃ³dulos del proyecto
â”‚   â”œâ”€â”€ airflow-orchestration/            # OrquestaciÃ³n con Airflow + dbt
â”‚   â”‚   â”œâ”€â”€ dags/                         # DAGs de Airflow
â”‚   â”‚   â”œâ”€â”€ include/                      # CÃ³digo compartido (dbt, sql, config)
â”‚   â”‚   â”œâ”€â”€ plugins/                      # Plugins de Airflow
â”‚   â”‚   â”œâ”€â”€ tests/                        # Tests del mÃ³dulo
â”‚   â”‚   â”œâ”€â”€ Dockerfile                    # Imagen de Astronomer
â”‚   â”‚   â”œâ”€â”€ requirements.txt              # Dependencias Python
â”‚   â”‚   â”œâ”€â”€ Makefile                      # Comandos del mÃ³dulo
â”‚   â”‚   â””â”€â”€ README.md                     # DocumentaciÃ³n
â”‚   â”‚
â”‚   â””â”€â”€ aws-bedrock-agents/               # Agentes AI con AWS Bedrock
â”‚       â”œâ”€â”€ infrastructure/               # IaC con Terraform
â”‚       â”œâ”€â”€ lambda-functions/             # Funciones Lambda
â”‚       â”œâ”€â”€ step-functions/               # Workflows
â”‚       â”œâ”€â”€ tests/                        # Tests del mÃ³dulo
â”‚       â””â”€â”€ README.md                     # DocumentaciÃ³n
â”‚
â”œâ”€â”€ docs/                                  # DocumentaciÃ³n general
â”‚   â”œâ”€â”€ architecture/                     # Diagramas y arquitectura
â”‚   â””â”€â”€ aws-bedrock-agents/               # Docs de Bedrock
â”‚       â”œâ”€â”€ README.md                     # Arquitectura detallada
â”‚       â””â”€â”€ COST_ANALYSIS.md              # AnÃ¡lisis de costos
â”‚
â”œâ”€â”€ .claude/                               # ConfiguraciÃ³n Claude Code
â”‚   â””â”€â”€ commands/                         # Comandos personalizados
â”‚
â”œâ”€â”€ CLAUDE.md                              # Instrucciones del proyecto
â”œâ”€â”€ README.md                              # Este archivo
â””â”€â”€ .gitignore
```

### MÃ³dulos Disponibles

#### 1. Airflow Orchestration
OrquestaciÃ³n de pipelines de datos con Apache Airflow y transformaciones con dbt.

**UbicaciÃ³n**: `modules/airflow-orchestration/`
**DocumentaciÃ³n**: `modules/airflow-orchestration/README.md`

#### 2. AWS Bedrock AI Agents
Agentes de AI para procesamiento y consulta de documentos usando AWS Bedrock.

**UbicaciÃ³n**: `modules/aws-bedrock-agents/`
**DocumentaciÃ³n**:
- MÃ³dulo: `modules/aws-bedrock-agents/README.md`
- Arquitectura: `docs/aws-bedrock-agents/README.md`
- Costos: `docs/aws-bedrock-agents/COST_ANALYSIS.md`
```

## ðŸš€ Quick Start

### MÃ³dulo Airflow Orchestration

**Prerrequisitos**:
- Docker Desktop
- Astronomer CLI: `brew install astro`
- Make (opcional, para usar atajos)

**Iniciar el mÃ³dulo**:
```bash
# Navegar al mÃ³dulo
cd modules/airflow-orchestration

# OpciÃ³n 1: Con Makefile
make start

# OpciÃ³n 2: Comando directo
astro dev start
```

### Acceder a Airflow

- **URL**: http://localhost:8080
- **Usuario**: `admin`
- **Password**: `admin`

### Cargar datos de ejemplo

**IMPORTANTE**: Ejecuta el DAG `setup_sample_database` primero para cargar datos de ejemplo:

1. Ve a http://localhost:8080
2. Busca el DAG `setup_sample_database`
3. ActÃ­valo y ejecÃºtalo (Trigger DAG)
4. Esto crea:
   - Schema `sample_data`
   - Tablas: `customers`, `products`, `orders`, `order_items`, `categories`
   - ~100 clientes, 25 productos, 200 Ã³rdenes con datos realistas

Una vez cargados los datos, los otros DAGs pueden trabajar con ellos.

## ðŸ“Š Base de Datos de Ejemplo

El schema `sample_data` contiene un e-commerce simplificado:

- **customers**: 100 clientes en diferentes paÃ­ses
- **categories**: 5 categorÃ­as de productos
- **products**: 25 productos con precios y stock
- **orders**: 200 Ã³rdenes con diferentes estados
- **order_items**: Detalles de cada orden

Los DAGs de ejemplo (`example_etl_products`, `example_postgres_crud`) usan estos datos.

## ðŸ“ Comandos Ãštiles

### GestiÃ³n del entorno

```bash
make start          # Levantar Airflow
make stop           # Detener Airflow
make restart        # Reiniciar Airflow
make kill           # Detener y eliminar contenedores
make clean          # Limpiar volÃºmenes y datos
make logs           # Ver logs
make shell          # Abrir shell en el contenedor
```

### dbt

```bash
make dbt-debug      # Verificar configuraciÃ³n de dbt
make dbt-run        # Ejecutar modelos de dbt
make dbt-test       # Ejecutar tests de dbt
make dbt-compile    # Compilar modelos de dbt
```

### Tests

```bash
make pytest         # Ejecutar tests de Airflow
```

## ðŸ”§ ConfiguraciÃ³n de dbt

**dbt viene preinstalado y configurado** automÃ¡ticamente al levantar el proyecto.

El proyecto dbt estÃ¡ en `include/dbt/` y se conecta a PostgreSQL usando las variables de entorno en `.env`:

```bash
DBT_HOST=postgres
DBT_USER=postgres
DBT_PASSWORD=postgres
DBT_DATABASE=postgres
DBT_SCHEMA=public
DBT_PORT=5432
```

### DAG de dbt

`dbt_example_dag` ejecuta automÃ¡ticamente:
1. **dbt debug**: Verifica la configuraciÃ³n y conexiÃ³n
2. **dbt run**: Materializa los modelos (staging â†’ marts)
3. **dbt test**: Ejecuta tests de validaciÃ³n

### Estructura de modelos

- **staging/**: Modelos que limpian y estandarizan datos raw (views)
  - `stg_example.sql`: Ejemplo de modelo staging
- **marts/**: Modelos finales para anÃ¡lisis y reporting (tables)
  - `fct_example.sql`: Ejemplo de tabla de hechos

## ðŸ“¦ Agregar Dependencias

### Python

1. Editar `requirements.txt` para dependencias de Python
2. Ejecutar `astro dev restart`

**Nota**: dbt-postgres estÃ¡ instalado en el `Dockerfile` para evitar conflictos de dependencias.

### Sistema

1. Editar `packages.txt` con paquetes de Ubuntu (ej: `git`)
2. Ejecutar `astro dev restart`

### dbt

Para actualizar la versiÃ³n de dbt, editar la lÃ­nea correspondiente en el `Dockerfile`:

```dockerfile
RUN pip install --no-cache-dir "dbt-postgres>=1.9.0,<2.0.0"
```

## ðŸ§ª Desarrollo

### Agregar un nuevo DAG

1. Crear archivo en `dags/`
2. El DAG aparecerÃ¡ automÃ¡ticamente en la UI (hot-reload)

### Agregar modelos dbt

1. Crear archivos `.sql` en `include/dbt/models/staging/` o `marts/`
2. Ejecutar `make dbt-run` para materializarlos

### Tests de DAGs

Crear tests en `tests/dags/` y ejecutar con `make pytest`

## ðŸ› Troubleshooting

### El DAG no aparece en la UI

```bash
# Ver logs del scheduler
make logs-scheduler

# Verificar que no haya errores de sintaxis
astro dev bash -c "python dags/tu_dag.py"
```

### Problemas con dbt

```bash
# Verificar configuraciÃ³n
make dbt-debug

# Ver logs detallados
make logs
```

### Resetear todo

```bash
make clean
make start
```

## âœ¨ Buenas PrÃ¡cticas Implementadas

Este repositorio sigue principios de **DRY** (Don't Repeat Yourself) y **KISS** (Keep It Simple):

### SQL Externalizado

âœ… **Queries SQL en archivos separados** (`include/sql/`)
- FÃ¡cil de mantener y versionar
- Reutilizable entre DAGs
- Testeable independientemente
- Git diff mÃ¡s claro

```python
# Mal âŒ
sql = "SELECT * FROM table WHERE..."

# Bien âœ…
sql = read_sql_file('include/sql/analytics/query.sql')
```

### ConfiguraciÃ³n en YAML

âœ… **ConfiguraciÃ³n externalizada** (`include/config/dag_config.yaml`)
- Un solo lugar para cambiar configuraciones
- FÃ¡cil de entender y modificar
- Versionable y documentable

```python
# Cargar config
with open('include/config/dag_config.yaml') as f:
    config = yaml.safe_load(f)
```

### Estructura Organizada

âœ… **SeparaciÃ³n clara de concerns**:
- `dags/`: LÃ³gica de orquestaciÃ³n
- `include/sql/`: Queries SQL
- `include/config/`: Configuraciones
- `include/dbt/`: Modelos de transformaciÃ³n

### DAGs como Ejemplos

Cada DAG tiene un nombre descriptivo que indica su propÃ³sito:

**Setup:**
- `setup_sample_database`: Inicializa base de datos con datos de prueba

**Ejemplos:**
- `example_etl_products`: ETL de productos con buenas prÃ¡cticas
- `example_postgres_crud`: Operaciones CRUD con PostgreSQL
- `example_conditional_branching`: Branching basado en calidad de datos

**dbt:**
- `dbt_run_transformations`: Ejecuta modelos dbt (debug â†’ run â†’ test)

### dbt Tests

Los modelos dbt incluyen tests de calidad de datos:

```bash
# Ejecutar tests de dbt
make dbt-test

# O manualmente
astro dev bash -c "cd include/dbt && dbt test"
```

**Tests implementados:**
- `unique`: Verifica valores Ãºnicos en columnas clave
- `not_null`: Asegura que columnas crÃ­ticas no sean nulas
- `expression_is_true`: Valida reglas de negocio personalizadas

Los tests se definen en `include/dbt/models/*/schema.yml`

## ðŸ¤– MÃ³dulo AWS Bedrock AI Agents

### DescripciÃ³n

SoluciÃ³n modular para crear agentes de AI usando AWS Bedrock que procesan y responden consultas sobre diversos tipos de archivos (PDFs, documentos, CSVs, cÃ³digo, imÃ¡genes).

### Arquitectura

- **Document Ingestion Pipeline**: S3 â†’ EventBridge â†’ Step Functions â†’ Lambda â†’ S3 processed â†’ Knowledge Base
- **AI Agent**: Bedrock Agent + Knowledge Bases (RAG) + Lambda custom actions
- **Multi-ambiente**: dev/staging/prod con Terraform

### Casos de Uso

- AnÃ¡lisis de documentos y contratos
- Code assistant para bases de cÃ³digo
- Data analysis sobre datasets
- Document processing multi-fuente

### Inicio RÃ¡pido

```bash
# 1. Revisar anÃ¡lisis de costos PRIMERO
cat docs/aws-bedrock-agents/COST_ANALYSIS.md

# 2. Ver documentaciÃ³n completa
cat modules/aws-bedrock-agents/README.md

# 3. Desplegar infraestructura (dev)
cd modules/aws-bedrock-agents/infrastructure/terraform/environments/dev
terraform init
terraform plan
terraform apply
```

### Estado

ðŸ”„ **En desarrollo**
- âœ… Arquitectura diseÃ±ada
- âœ… Estructura de directorios creada
- âœ… DocumentaciÃ³n base
- â³ MÃ³dulos de Terraform
- â³ Lambda functions
- â³ Step Functions workflows

Ver documentaciÃ³n completa: `docs/aws-bedrock-agents/README.md`

## ðŸ“š Recursos

- [Astronomer Docs](https://www.astronomer.io/docs/)
- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [dbt Docs](https://docs.getdbt.com/)

## ðŸ¤ Contribuir

1. Crear una rama desde `main`
2. Hacer cambios y testear localmente
3. Crear Pull Request

## ðŸ“„ Licencia

[Definir licencia]
