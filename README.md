# Snail Data Solutions

Repositorio de soluciones de datos usando Apache Airflow y dbt, gestionado con Astronomer.

## ðŸ› ï¸ Stack TecnolÃ³gico

- **Airflow**: 2.10.3 (Astro Runtime 12.5.0)
- **dbt**: 1.10.15 con adaptador PostgreSQL 1.9.1 (instalado automÃ¡ticamente)
- **PostgreSQL**: 13
- **Astronomer CLI**: Para desarrollo local
- **Git**: Incluido para operaciones de dbt

## ðŸ“ Estructura del Proyecto

```
snail-data-solutions/
â”œâ”€â”€ dags/                          # DAGs de Airflow
â”‚   â”œâ”€â”€ default_dag.py            # DAG de ejemplo bÃ¡sico
â”‚   â”œâ”€â”€ dbt_example_dag.py        # DAG que ejecuta modelos dbt
â”‚   â”œâ”€â”€ seed_database.py          # Carga datos de ejemplo
â”‚   â”œâ”€â”€ etl_taskflow_example.py   # ETL con TaskFlow API
â”‚   â”œâ”€â”€ etl_taskflow_refactored.py # ETL refactorizado (buenas prÃ¡cticas)
â”‚   â”œâ”€â”€ postgres_example.py       # Operaciones PostgreSQL
â”‚   â””â”€â”€ conditional_example.py    # Branching condicional
â”œâ”€â”€ include/                       # CÃ³digo compartido
â”‚   â”œâ”€â”€ dbt/                      # Proyecto dbt
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”‚   â””â”€â”€ profiles.yml
â”‚   â”œâ”€â”€ sql/                      # Queries SQL externalizados
â”‚   â”‚   â”œâ”€â”€ seed/                 # Scripts de inicializaciÃ³n
â”‚   â”‚   â”‚   â”œâ”€â”€ 01_create_schema.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ 02_create_tables.sql
â”‚   â”‚   â”‚   â””â”€â”€ 03_insert_sample_data.sql
â”‚   â”‚   â”œâ”€â”€ etl/                  # Queries ETL
â”‚   â”‚   â””â”€â”€ analytics/            # Queries de anÃ¡lisis
â”‚   â””â”€â”€ config/                   # Configuraciones YAML
â”‚       â””â”€â”€ dag_config.yaml       # ConfiguraciÃ³n de DAGs
â”œâ”€â”€ plugins/                       # Plugins de Airflow
â”œâ”€â”€ tests/                         # Tests
â”‚   â””â”€â”€ dags/                     # Tests de DAGs
â”œâ”€â”€ Dockerfile                     # Imagen base de Astronomer
â”œâ”€â”€ requirements.txt               # Dependencias Python
â”œâ”€â”€ packages.txt                   # Paquetes del sistema
â”œâ”€â”€ airflow_settings.yaml          # ConfiguraciÃ³n local
â””â”€â”€ Makefile                      # Comandos Ãºtiles
```

## ðŸš€ Quick Start

### Prerrequisitos

- Docker Desktop
- Astronomer CLI: `brew install astro`
- Make (opcional, para usar atajos)

### Iniciar el proyecto

```bash
# OpciÃ³n 1: Con Makefile
make start

# OpciÃ³n 2: Comando directo
astro dev start
```

### Acceder a Airflow

- **URL**: http://localhost:8080
- **Usuario**: `admin`
- **Password**: `admin`

### Cargar datos de ejemplo

**IMPORTANTE**: Ejecuta el DAG `seed_database` primero para cargar datos de ejemplo:

1. Ve a http://localhost:8080
2. Busca el DAG `seed_database`
3. ActÃ­valo y ejecÃºtalo (Trigger DAG)
4. Esto crea:
   - Schema `sample_data`
   - Tablas: `customers`, `products`, `orders`, `order_items`, `categories`
   - ~100 clientes, 25 productos, 200 Ã³rdenes con datos realistas

Una vez cargados los datos, los otros DAGs pueden trabajar con ellos.

## ðŸ“Š Base de Datos de Ejemplo

El schema `sample_data` contiene un e-commerce simplificado:

- **customers**: 100 clientes en diferentes paÃ­ses
- **categories**: 5 categorÃ­as de productos
- **products**: 25 productos con precios y stock
- **orders**: 200 Ã³rdenes con diferentes estados
- **order_items**: Detalles de cada orden

Los DAGs de ejemplo (`etl_taskflow_refactored`, `postgres_example`) usan estos datos.

## ðŸ“ Comandos Ãštiles

### GestiÃ³n del entorno

```bash
make start          # Levantar Airflow
make stop           # Detener Airflow
make restart        # Reiniciar Airflow
make kill           # Detener y eliminar contenedores
make clean          # Limpiar volÃºmenes y datos
make logs           # Ver logs
make shell          # Abrir shell en el contenedor
```

### dbt

```bash
make dbt-debug      # Verificar configuraciÃ³n de dbt
make dbt-run        # Ejecutar modelos de dbt
make dbt-test       # Ejecutar tests de dbt
make dbt-compile    # Compilar modelos de dbt
```

### Tests

```bash
make pytest         # Ejecutar tests de Airflow
```

## ðŸ”§ ConfiguraciÃ³n de dbt

**dbt viene preinstalado y configurado** automÃ¡ticamente al levantar el proyecto.

El proyecto dbt estÃ¡ en `include/dbt/` y se conecta a PostgreSQL usando las variables de entorno en `.env`:

```bash
DBT_HOST=postgres
DBT_USER=postgres
DBT_PASSWORD=postgres
DBT_DATABASE=postgres
DBT_SCHEMA=public
DBT_PORT=5432
```

### DAG de dbt

`dbt_example_dag` ejecuta automÃ¡ticamente:
1. **dbt debug**: Verifica la configuraciÃ³n y conexiÃ³n
2. **dbt run**: Materializa los modelos (staging â†’ marts)
3. **dbt test**: Ejecuta tests de validaciÃ³n

### Estructura de modelos

- **staging/**: Modelos que limpian y estandarizan datos raw (views)
  - `stg_example.sql`: Ejemplo de modelo staging
- **marts/**: Modelos finales para anÃ¡lisis y reporting (tables)
  - `fct_example.sql`: Ejemplo de tabla de hechos

## ðŸ“¦ Agregar Dependencias

### Python

1. Editar `requirements.txt` para dependencias de Python
2. Ejecutar `astro dev restart`

**Nota**: dbt-postgres estÃ¡ instalado en el `Dockerfile` para evitar conflictos de dependencias.

### Sistema

1. Editar `packages.txt` con paquetes de Ubuntu (ej: `git`)
2. Ejecutar `astro dev restart`

### dbt

Para actualizar la versiÃ³n de dbt, editar la lÃ­nea correspondiente en el `Dockerfile`:

```dockerfile
RUN pip install --no-cache-dir "dbt-postgres>=1.9.0,<2.0.0"
```

## ðŸ§ª Desarrollo

### Agregar un nuevo DAG

1. Crear archivo en `dags/`
2. El DAG aparecerÃ¡ automÃ¡ticamente en la UI (hot-reload)

### Agregar modelos dbt

1. Crear archivos `.sql` en `include/dbt/models/staging/` o `marts/`
2. Ejecutar `make dbt-run` para materializarlos

### Tests de DAGs

Crear tests en `tests/dags/` y ejecutar con `make pytest`

## ðŸ› Troubleshooting

### El DAG no aparece en la UI

```bash
# Ver logs del scheduler
make logs-scheduler

# Verificar que no haya errores de sintaxis
astro dev bash -c "python dags/tu_dag.py"
```

### Problemas con dbt

```bash
# Verificar configuraciÃ³n
make dbt-debug

# Ver logs detallados
make logs
```

### Resetear todo

```bash
make clean
make start
```

## âœ¨ Buenas PrÃ¡cticas Implementadas

Este repositorio sigue principios de **DRY** (Don't Repeat Yourself) y **KISS** (Keep It Simple):

### SQL Externalizado

âœ… **Queries SQL en archivos separados** (`include/sql/`)
- FÃ¡cil de mantener y versionar
- Reutilizable entre DAGs
- Testeable independientemente
- Git diff mÃ¡s claro

```python
# Mal âŒ
sql = "SELECT * FROM table WHERE..."

# Bien âœ…
sql = read_sql_file('include/sql/analytics/query.sql')
```

### ConfiguraciÃ³n en YAML

âœ… **ConfiguraciÃ³n externalizada** (`include/config/dag_config.yaml`)
- Un solo lugar para cambiar configuraciones
- FÃ¡cil de entender y modificar
- Versionable y documentable

```python
# Cargar config
with open('include/config/dag_config.yaml') as f:
    config = yaml.safe_load(f)
```

### Estructura Organizada

âœ… **SeparaciÃ³n clara de concerns**:
- `dags/`: LÃ³gica de orquestaciÃ³n
- `include/sql/`: Queries SQL
- `include/config/`: Configuraciones
- `include/dbt/`: Modelos de transformaciÃ³n

### DAGs como Ejemplos

Cada DAG demuestra un patrÃ³n diferente:
- `seed_database`: InicializaciÃ³n de datos
- `etl_taskflow_refactored`: ETL con buenas prÃ¡cticas
- `postgres_example`: Operaciones SQL directas
- `conditional_example`: LÃ³gica condicional
- `dbt_example_dag`: Transformaciones con dbt

## ðŸ“š Recursos

- [Astronomer Docs](https://www.astronomer.io/docs/)
- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [dbt Docs](https://docs.getdbt.com/)

## ðŸ¤ Contribuir

1. Crear una rama desde `main`
2. Hacer cambios y testear localmente
3. Crear Pull Request

## ðŸ“„ Licencia

[Definir licencia]
