# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

---

# Instrucciones del Proyecto Snail Data Solutions

## Sobre Snail Data Solutions

**Snail Data Solutions** es una consultora especializada en Data Engineering y AI. Este repositorio contiene soluciones, proyectos, templates y playgrounds reutilizables para acelerar implementaciones de clientes y servir como base de conocimiento.

## Quick Reference

### Common Commands (Airflow Module)
```bash
# Working directory
cd modules/airflow-orchestration

# Core operations
make start          # Start Airflow (http://localhost:8080, admin/admin)
make stop           # Stop Airflow
make logs           # View all logs
make shell          # Open shell in container

# dbt operations
make dbt-run        # Run dbt models
make dbt-test       # Run dbt tests
make dbt-debug      # Verify dbt configuration

# Testing
make pytest         # Run Airflow tests

# Single test
astro dev pytest tests/dags/test_specific_dag.py
```

### Architecture at a Glance
- **Airflow Orchestration Module**: `modules/airflow-orchestration/`
  - DAGs: `dags/` (setup_*, example_*, dbt_*)
  - dbt models: `include/dbt/models/` (staging/, marts/)
  - SQL queries: `include/sql/`
  - Config: `include/config/`

- **AWS Bedrock Module**: `modules/aws-bedrock-agents/` (in development)
  - Infrastructure: `infrastructure/terraform/`
  - Lambda functions: `lambda-functions/`
  - Docs: `docs/aws-bedrock-agents/`

### Key Files to Read First
- This file (CLAUDE.md) for project context
- `modules/airflow-orchestration/README.md` for Airflow setup
- `modules/aws-bedrock-agents/README.md` for Bedrock module
- `docs/aws-bedrock-agents/COST_ANALYSIS.md` for AWS cost estimates

## Objetivo del Repositorio

Este repositorio funciona como:
- **Biblioteca de soluciones**: Templates y patrones probados listos para usar
- **Playground**: Espacio para experimentar con nuevas tecnolog√≠as y patrones
- **Base de conocimiento**: Ejemplos y documentaci√≥n de mejores pr√°cticas
- **Acelerador de proyectos**: C√≥digo reutilizable para implementaciones de clientes

## Stack Tecnol√≥gico

### Actual
- **Orquestaci√≥n**: Apache Airflow 2.10.3 (Astro Runtime 12.5.0)
- **Transformaci√≥n**: dbt 1.10.15 con adaptador PostgreSQL
- **Plataforma**: Astronomer (desarrollo local y deployment)
- **Cloud**: AWS (Bedrock, Lambda, Step Functions, S3, Textract)
- **Base de Datos**: PostgreSQL 13 (para ejemplos locales)
- **Contenedores**: Docker
- **IaC**: Terraform (multi-ambiente: dev/staging/prod)

### Pr√≥ximamente
- **Databricks**: Para procesamiento de big data y ML

## Principios y Valores del Proyecto

Todo c√≥digo y arquitectura debe seguir estos principios:

### 1. Excelencia Operativa
- C√≥digo limpio, legible y bien documentado
- Automatizaci√≥n de procesos repetitivos
- Monitoreo y observabilidad desde el dise√±o
- Documentaci√≥n siempre actualizada

### 2. Seguridad
- Credenciales NUNCA en c√≥digo (usar variables de entorno, secrets managers)
- Principio de privilegios m√≠nimos
- Validaci√≥n de inputs y outputs
- Logs sin informaci√≥n sensible

### 3. Confiabilidad y Fiabilidad
- Tests unitarios y de integraci√≥n
- Manejo de errores robusto
- Idempotencia en todas las operaciones
- Retry logic con backoff exponencial

### 4. Optimizaci√≥n de Costos
- Recursos dimensionados apropiadamente
- Limpieza de recursos temporales
- Monitoreo de uso de recursos
- Cacheo inteligente cuando aplique

### 5. Rendimiento Eficiente
- Queries optimizadas
- Procesamiento en paralelo cuando sea posible
- Lazy loading y streaming para grandes vol√∫menes
- √çndices apropiados en bases de datos

### 6. Sostenibilidad y Escalabilidad
- Arquitectura modular y desacoplada
- Configuraci√≥n externalizada
- Dise√±o para escalar horizontalmente
- Abstracciones reutilizables

### 7. Reusabilidad
- C√≥digo DRY (Don't Repeat Yourself)
- Templates y funciones compartidas
- Convenciones claras y consistentes
- Documentaci√≥n de casos de uso

### 8. Principios de Programaci√≥n
- SOLID principles
- KISS (Keep It Simple, Stupid)
- YAGNI (You Aren't Gonna Need It)
- Separation of Concerns
- Single Responsibility

### 9. Infraestructura como C√≥digo
- Todo infrastructure debe ser c√≥digo
- Versionado en Git
- Ambientes reproducibles
- Deployment automatizado

## Arquitectura Modular

El proyecto est√° dise√±ado para ser **completamente modular**. Puedes levantar componentes espec√≠ficos sin necesidad de correr todo el stack.

### Estructura de M√≥dulos

```
snail-data-solutions/
‚îú‚îÄ‚îÄ modules/                        # Todos los m√≥dulos del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ airflow-orchestration/     # M√≥dulo de orquestaci√≥n con Airflow + dbt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dags/                  # DAGs de Airflow
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setup_*            # DAGs de setup/inicializaci√≥n
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ example_*          # DAGs de ejemplo/referencia
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dbt_*              # DAGs que ejecutan dbt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ include/               # C√≥digo compartido
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dbt/               # Proyecto dbt
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sql/               # SQL queries
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config/            # Configuraciones YAML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plugins/               # Plugins de Airflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tests/                 # Tests del m√≥dulo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile             # Imagen de Astronomer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt       # Dependencias Python
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Makefile              # Comandos del m√≥dulo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md              # Documentaci√≥n del m√≥dulo
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ aws-bedrock-agents/        # M√≥dulo de agentes AI con AWS Bedrock
‚îÇ       ‚îú‚îÄ‚îÄ infrastructure/        # IaC con Terraform
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ terraform/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ modules/       # M√≥dulos reutilizables
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ environments/  # dev/staging/prod
‚îÇ       ‚îú‚îÄ‚îÄ lambda-functions/      # C√≥digo de Lambdas
‚îÇ       ‚îú‚îÄ‚îÄ step-functions/        # Workflows
‚îÇ       ‚îú‚îÄ‚îÄ tests/                 # Tests del m√≥dulo
‚îÇ       ‚îî‚îÄ‚îÄ README.md              # Documentaci√≥n del m√≥dulo
‚îÇ
‚îú‚îÄ‚îÄ docs/                           # Documentaci√≥n general del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ architecture/              # Diagramas y arquitectura
‚îÇ   ‚îî‚îÄ‚îÄ aws-bedrock-agents/        # Docs espec√≠ficos de Bedrock
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ       ‚îî‚îÄ‚îÄ COST_ANALYSIS.md
‚îÇ
‚îú‚îÄ‚îÄ .claude/                        # Configuraci√≥n de Claude Code
‚îÇ   ‚îî‚îÄ‚îÄ commands/
‚îÇ       ‚îî‚îÄ‚îÄ init.md
‚îÇ
‚îú‚îÄ‚îÄ CLAUDE.md                       # Este archivo
‚îú‚îÄ‚îÄ README.md                       # README principal
‚îî‚îÄ‚îÄ .gitignore
```

### C√≥mo Levantar Componentes Espec√≠ficos

**M√≥dulo Airflow Orchestration:**
```bash
# Desde el root del proyecto
cd modules/airflow-orchestration
astro dev start

# O usando make
make start

# Solo dbt (dentro del contenedor)
make dbt-run
```

**M√≥dulo AWS Bedrock Agents:**
```bash
# Ver an√°lisis de costos primero
cat docs/aws-bedrock-agents/COST_ANALYSIS.md

# Desplegar infraestructura (ambiente dev)
cd modules/aws-bedrock-agents/infrastructure/terraform/environments/dev
terraform init
terraform plan
terraform apply

# Ver documentaci√≥n completa
cat modules/aws-bedrock-agents/README.md
```

**DAGs espec√≠ficos:**
Los DAGs se pueden activar/desactivar individualmente en la UI de Airflow o mediante tags.

## M√≥dulos del Proyecto

### M√≥dulo AWS Bedrock AI Agents

**Descripci√≥n**: Soluci√≥n modular para crear agentes de AI usando AWS Bedrock que procesan y responden consultas sobre diversos tipos de archivos.

**Componentes**:
- Amazon Bedrock (Claude/Titan) para modelos de lenguaje
- Knowledge Bases for Amazon Bedrock (RAG)
- AWS Lambda para procesamiento de documentos
- AWS Step Functions para orquestaci√≥n de workflows
- Amazon S3 para almacenamiento (raw ‚Üí processed)
- Amazon Textract para OCR
- Terraform para IaC multi-ambiente

**Tipos de archivos soportados**:
- PDFs y documentos
- Datos estructurados (CSV, JSON)
- C√≥digo fuente
- Multimedia (im√°genes con texto v√≠a OCR)

**Casos de uso**:
- An√°lisis de documentos y contratos
- Code assistant para bases de c√≥digo
- Data analysis sobre datasets
- Document processing multi-fuente

**Arquitectura**:
- Pipeline de ingesta: S3 ‚Üí EventBridge ‚Üí Step Functions ‚Üí Lambda ‚Üí S3 processed ‚Üí Knowledge Base
- Agente AI: Bedrock Agent + Knowledge Base + Lambda custom actions
- Multi-ambiente: dev/staging/prod con Terraform

**Documentaci√≥n completa**:
- M√≥dulo: `modules/aws-bedrock-agents/README.md`
- Arquitectura: `docs/aws-bedrock-agents/README.md`
- Costos: `docs/aws-bedrock-agents/COST_ANALYSIS.md`

**Costos estimados mensuales** (ver an√°lisis completo en COST_ANALYSIS.md):
- MVP/Testing: **$10-30** (Claude Haiku + Pinecone free tier)
- Producci√≥n ligera: **$120-200** (Claude Sonnet + Aurora pgvector)
- Producci√≥n moderada: **$350-450** (Sonnet + OpenSearch 2 OCU)
- Producci√≥n intensiva: **$800-1,200** (Sonnet/Opus + OpenSearch escalado)

‚ö†Ô∏è **Nota**: El costo principal es el vector store (OpenSearch ~$175/mes m√≠nimo). Para minimizar costos iniciales, usar alternativas como Aurora pgvector ($50-80/mes) o Pinecone free tier.

**Estado**: üîÑ En desarrollo
- ‚úÖ Arquitectura dise√±ada
- ‚úÖ Estructura de directorios creada
- ‚úÖ Documentaci√≥n base
- ‚úÖ An√°lisis completo de costos
- ‚úÖ Estructura modular documentada
- ‚è≥ M√≥dulos de Terraform
- ‚è≥ Lambda functions
- ‚è≥ Step Functions workflows

## Convenciones del Proyecto

### Nomenclatura

**DAGs:**
- Prefijos: `setup_`, `example_`, `dbt_`, `etl_`, `ml_`
- Formato: `{prefix}_{descripcion_snake_case}.py`
- Ejemplos: `setup_sample_database.py`, `etl_customer_orders.py`

**Modelos dbt:**
- Staging: `stg_{source}_{entity}.sql` (ej: `stg_postgres_customers.sql`)
- Marts: `{tipo}_{descripcion}.sql` (ej: `fct_sales.sql`, `dim_customers.sql`)

**Archivos SQL:**
- Formato: `{numero}_{descripcion}.sql` si son secuenciales
- Formato: `{descripcion}.sql` si son independientes

**Variables de entorno:**
- May√∫sculas con underscores: `DBT_HOST`, `AIRFLOW_CONN_POSTGRES`

**M√≥dulos de Terraform:**
- Formato: `{servicio}-{proposito}` (ej: `bedrock-agent`, `lambda-processor`)
- Variables: snake_case (ej: `knowledge_base_name`, `lambda_timeout`)
- Outputs: snake_case con sufijo descriptivo (ej: `bucket_arn`, `lambda_function_name`)

**Lambda Functions:**
- Directorio: `{tipo}-{proposito}` (ej: `pdf-processor`, `query-handler`)
- Handler: `handler.py` con funci√≥n `lambda_handler`
- Archivos: snake_case (ej: `pdf_extractor.py`, `text_processor.py`)

**Step Functions:**
- Archivos: `{workflow}-{proposito}.asl.json` (ej: `document-ingestion.asl.json`)
- Estados: PascalCase (ej: `ProcessDocument`, `IndexContent`)

### Organizaci√≥n de C√≥digo

**SQL Externalizado:**
- NUNCA escribir SQL hardcoded en Python
- Todo SQL debe estar en `include/sql/` o modelos dbt
- Usar funciones helper para leer archivos SQL

**Configuraci√≥n Externalizada:**
- Par√°metros en archivos YAML en `include/config/`
- Variables de entorno en `.env` (nunca commiteadas)
- `.env.example` siempre actualizado

**Secrets y Credenciales:**
- NUNCA en c√≥digo o configs commiteados
- Usar `.env` local (en `.gitignore`)
- Usar Airflow Connections/Variables en producci√≥n
- Usar AWS Secrets Manager en cloud

### Tests

**Obligatorios para:**
- Todos los DAGs nuevos (test de import m√≠nimo)
- Todos los modelos dbt (unique, not_null como m√≠nimo)
- Funciones compartidas/helpers

**Ubicaci√≥n:**
- Tests de DAGs: `tests/dags/`
- Tests de dbt: `include/dbt/models/*/schema.yml`
- Tests de helpers: `tests/unit/`

### Documentaci√≥n

**README.md:**
- Informaci√≥n para usuarios/developers
- Quick start y comandos b√°sicos
- Troubleshooting com√∫n

**CLAUDE.md (este archivo):**
- Contexto del proyecto para Claude
- Principios y valores
- Instrucciones espec√≠ficas para desarrollo
- **DEBE actualizarse en cada cambio significativo**

**Docstrings:**
- Todas las funciones p√∫blicas
- Todos los DAGs (en el docstring del DAG)
- Modelos dbt (en archivos `.yml`)

## Instrucciones para Claude

### Typical Development Workflows

#### Adding a New DAG
1. Read existing DAGs in `modules/airflow-orchestration/dags/` to understand patterns
2. Create new DAG file with appropriate prefix (setup_*, example_*, etl_*, etc.)
3. Externalize SQL queries to `include/sql/`
4. Externalize configuration to `include/config/` (YAML)
5. Add test in `tests/dags/test_your_dag.py`
6. Run: `cd modules/airflow-orchestration && make start`
7. Verify DAG appears in UI at http://localhost:8080
8. Run tests: `make pytest`

#### Adding a dbt Model
1. Navigate to `modules/airflow-orchestration/include/dbt/models/`
2. Create staging model in `staging/stg_{source}_{entity}.sql`
3. Create mart model in `marts/{fct|dim}_{description}.sql`
4. Add tests in corresponding `schema.yml` file (unique, not_null minimum)
5. Run: `make dbt-run` to materialize
6. Run: `make dbt-test` to validate
7. Update documentation in schema.yml

#### Modifying AWS Bedrock Infrastructure
1. Read existing Terraform modules in `modules/aws-bedrock-agents/infrastructure/terraform/modules/`
2. Review cost analysis first: `docs/aws-bedrock-agents/COST_ANALYSIS.md`
3. Modify Terraform module or create new one
4. Update variables in `environments/dev/variables.tf`
5. Plan: `terraform plan` from environment directory
6. Apply: `terraform apply` (with user confirmation)
7. Document changes in module README and update costs if applicable

#### Running Single Test
```bash
cd modules/airflow-orchestration
astro dev pytest tests/dags/test_specific_dag.py::test_function_name -v
```

#### Debugging a DAG Issue
1. Check scheduler logs: `make logs-scheduler`
2. Verify DAG syntax: `astro dev bash -c "python dags/your_dag.py"`
3. Check Airflow UI for import errors
4. Verify SQL files exist in `include/sql/`
5. Check database connection: `make dbt-debug`

### Code Patterns to Follow

**SQL Externalization Pattern:**
```python
# Read SQL from file (see example DAGs)
def read_sql_file(filepath):
    with open(filepath, 'r') as f:
        return f.read()

# Usage in DAG
sql = read_sql_file('include/sql/analytics/query.sql')
```

**YAML Configuration Pattern:**
```python
# Load config from YAML (see example DAGs)
import yaml

with open('include/config/dag_config.yaml') as f:
    config = yaml.safe_load(f)

# Access config values
schedule = config['dag']['schedule']
```

**dbt Model Pattern:**
```sql
-- Staging: stg_source_entity.sql (materialized as view)
-- Clean and standardize raw data
with source as (
    select * from {{ source('postgres', 'raw_table') }}
)
select
    id,
    lower(trim(name)) as name_clean,
    created_at
from source

-- Marts: fct_entity.sql (materialized as table)
-- Business logic and aggregations
select
    d.id,
    d.name_clean,
    count(*) as total_count
from {{ ref('stg_source_entity') }} d
group by 1, 2
```

**Airflow Connection Pattern:**
```python
# Use Airflow connections, not hardcoded credentials
from airflow.providers.postgres.hooks.postgres import PostgresHook

pg_hook = PostgresHook(postgres_conn_id='postgres_default')
```

### Al Trabajar en Este Proyecto

1. **SIEMPRE lee el c√≥digo existente antes de modificar**
   - Entiende los patrones actuales mostrados arriba
   - Mant√©n consistencia con el estilo existente
   - Busca ejemplos similares en `dags/example_*.py`

2. **Sigue los principios del proyecto**
   - Revisa la secci√≥n "Principios y Valores" antes de proponer cambios
   - Optimiza para reusabilidad y escalabilidad
   - Aplica los patrones de c√≥digo establecidos

3. **Mant√©n la modularidad**
   - Cada DAG debe ser independiente
   - Cada m√≥dulo de dbt debe ser autocontenido
   - Usa configuraciones externalizadas

4. **Documenta todos los cambios**
   - Actualiza README.md si afecta uso/comandos
   - Actualiza CLAUDE.md si cambian principios/estructura
   - Agrega docstrings a c√≥digo nuevo
   - Actualiza archivos `.yml` de dbt

5. **Tests son obligatorios**
   - Agrega tests para c√≥digo nuevo
   - Verifica que tests existentes pasen
   - No hacer commits si los tests fallan

6. **Seguridad primero**
   - Nunca expongas credenciales
   - Valida inputs externos
   - Usa conexiones de Airflow para DBs

### Al Crear Nuevas Soluciones

1. **Identifica si es reusable**
   - Si s√≠: crear template gen√©rico en `/templates` (futuro)
   - Si no: documentar caso de uso espec√≠fico

2. **Sigue nomenclatura establecida**
   - Usa prefijos apropiados
   - Nombres descriptivos y claros

3. **Externaliza configuraci√≥n**
   - Par√°metros en YAML
   - Credenciales en variables de entorno
   - Documentar configuraci√≥n en README

4. **Agrega ejemplos y tests**
   - Al menos un ejemplo de uso
   - Tests b√°sicos de funcionamiento

### Al Actualizar Documentaci√≥n

**Actualiza CLAUDE.md cuando:**
- Se agreguen nuevos m√≥dulos/componentes
- Cambien principios o convenciones
- Se agreguen nuevas herramientas al stack
- Se modifique la estructura del proyecto

**Actualiza README.md cuando:**
- Cambien comandos o instrucciones de uso
- Se agreguen nuevas dependencias
- Cambien los pasos de setup
- Se agreguen nuevos DAGs relevantes

### Common Issues and Solutions

**Issue: DAG not appearing in Airflow UI**
```bash
# Check scheduler logs for import errors
make logs-scheduler

# Verify DAG syntax
astro dev bash -c "python dags/your_dag.py"

# Common causes:
# - Syntax errors in DAG file
# - Missing dependencies in requirements.txt
# - Import errors (missing modules)
```

**Issue: dbt models failing**
```bash
# Verify dbt connection
make dbt-debug

# Check compiled SQL
make dbt-compile

# Common causes:
# - Database connection issues (check .env)
# - Missing source tables (run setup_sample_database DAG first)
# - Invalid Jinja syntax in models
# - Missing dependencies in dbt_project.yml
```

**Issue: "Permission denied" or database connection errors**
```bash
# Verify environment variables
astro dev bash -c "env | grep DBT"

# Restart Airflow to reload .env
make restart

# Common causes:
# - .env file not loaded
# - PostgreSQL container not running
# - Incorrect credentials in .env
```

**Issue: Changes not reflected after editing code**
```bash
# For DAG changes: Wait ~30 seconds (auto-reload)
# For include/ changes: Restart required
make restart

# For dbt changes: Recompile
make dbt-compile
```

**Issue: Docker resource issues**
```bash
# Clean up Docker resources
make clean

# Remove all volumes and start fresh
docker system prune -a --volumes

# Restart from scratch
make start
```

## Estado Actual del Proyecto

### Implementado
- ‚úÖ Setup de Airflow con Astronomer
- ‚úÖ Integraci√≥n de dbt con PostgreSQL
- ‚úÖ Base de datos de ejemplo (e-commerce)
- ‚úÖ DAGs de ejemplo (ETL, CRUD, branching)
- ‚úÖ Estructura modular con carpeta `modules/`
- ‚úÖ SQL y configs externalizados
- ‚úÖ Tests b√°sicos de DAGs
- ‚úÖ Tests de dbt con validaciones
- ‚úÖ Makefile con comandos √∫tiles
- ‚úÖ Documentaci√≥n en README.md
- ‚úÖ Documentaci√≥n CLAUDE.md con principios y convenciones
- ‚úÖ Comando `/init` para cargar contexto autom√°ticamente
- ‚úÖ AWS CLI configurado y verificado

### En Progreso
- üîÑ M√≥dulo AWS Bedrock AI Agents
  - ‚úÖ Arquitectura dise√±ada con diagrama de flujo
  - ‚úÖ Estructura modular documentada (modules/aws-bedrock-agents/)
  - ‚úÖ Documentaci√≥n completa del m√≥dulo
  - ‚úÖ An√°lisis detallado de costos (MVP: $10-30/mes, Prod: $120-1,200/mes)
  - ‚úÖ Estrategias de optimizaci√≥n de costos identificadas
  - ‚úÖ Alternativas de vector store evaluadas (OpenSearch vs Aurora vs Pinecone)
  - ‚è≥ M√≥dulos de Terraform (bedrock, lambda, step-functions, s3, iam)
  - ‚è≥ Lambda functions para procesamiento de documentos
  - ‚è≥ Step Functions workflows

### Por Implementar
- ‚è≥ Templates reutilizables para DAGs comunes
- ‚è≥ CI/CD pipeline (GitHub Actions)
- ‚è≥ Integraci√≥n Airflow + AWS (S3, Redshift operators)
- ‚è≥ Databricks integration
- ‚è≥ Deployment a Astronomer Cloud
- ‚è≥ Monitoreo y alertas
- ‚è≥ Cat√°logo de datos (dbt docs)

## Comandos de Claude Code

### Iniciar una Nueva Sesi√≥n

Cuando abras una nueva ventana/sesi√≥n de Claude Code, ejecuta:

```
/init
```

Este comando carga autom√°ticamente todo el contexto del proyecto desde CLAUDE.md, incluyendo principios, convenciones, y arquitectura.

### Otros Comandos √ötiles

```
/context        # Ver qu√© contexto est√° cargado actualmente
/help           # Ver todos los comandos disponibles
```

## Comandos del Proyecto

```bash
# Desarrollo
make start              # Levantar todo el stack
make stop               # Detener todo
make restart            # Reiniciar
make logs               # Ver logs

# dbt
make dbt-debug          # Verificar config dbt
make dbt-run            # Ejecutar modelos
make dbt-test           # Ejecutar tests dbt

# Testing
make pytest             # Tests de Airflow

# Limpieza
make clean              # Limpiar todo y empezar fresh
```

## Recursos

- [Documentaci√≥n Airflow](https://airflow.apache.org/docs/)
- [Documentaci√≥n dbt](https://docs.getdbt.com/)
- [Astronomer Docs](https://www.astronomer.io/docs/)
- [AWS Best Practices](https://aws.amazon.com/architecture/well-architected/)

---

**√öltima actualizaci√≥n**: 2025-11-24
**Mantenedor**: Snail Data Solutions
**Versi√≥n**: 1.1.0
