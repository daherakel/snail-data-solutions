# CLAUDE.md - Instrucciones del Proyecto Snail Data Solutions

## Sobre Snail Data Solutions

**Snail Data Solutions** es una consultora especializada en Data Engineering y AI. Este repositorio contiene soluciones, proyectos, templates y playgrounds reutilizables para acelerar implementaciones de clientes y servir como base de conocimiento.

## Objetivo del Repositorio

Este repositorio funciona como:
- **Biblioteca de soluciones**: Templates y patrones probados listos para usar
- **Playground**: Espacio para experimentar con nuevas tecnologÃ­as y patrones
- **Base de conocimiento**: Ejemplos y documentaciÃ³n de mejores prÃ¡cticas
- **Acelerador de proyectos**: CÃ³digo reutilizable para implementaciones de clientes

## Stack TecnolÃ³gico

### Actual
- **OrquestaciÃ³n**: Apache Airflow 2.10.3 (Astro Runtime 12.5.0)
- **TransformaciÃ³n**: dbt 1.10.15 con adaptador PostgreSQL
- **Plataforma**: Astronomer (desarrollo local y deployment)
- **Cloud**: AWS (Bedrock, Lambda, Step Functions, S3, Textract)
- **Base de Datos**: PostgreSQL 13 (para ejemplos locales)
- **Contenedores**: Docker
- **IaC**: Terraform (multi-ambiente: dev/staging/prod)

### PrÃ³ximamente
- **Databricks**: Para procesamiento de big data y ML

## Principios y Valores del Proyecto

Todo cÃ³digo y arquitectura debe seguir estos principios:

### 1. Excelencia Operativa
- CÃ³digo limpio, legible y bien documentado
- AutomatizaciÃ³n de procesos repetitivos
- Monitoreo y observabilidad desde el diseÃ±o
- DocumentaciÃ³n siempre actualizada

### 2. Seguridad
- Credenciales NUNCA en cÃ³digo (usar variables de entorno, secrets managers)
- Principio de privilegios mÃ­nimos
- ValidaciÃ³n de inputs y outputs
- Logs sin informaciÃ³n sensible

### 3. Confiabilidad y Fiabilidad
- Tests unitarios y de integraciÃ³n
- Manejo de errores robusto
- Idempotencia en todas las operaciones
- Retry logic con backoff exponencial

### 4. OptimizaciÃ³n de Costos
- Recursos dimensionados apropiadamente
- Limpieza de recursos temporales
- Monitoreo de uso de recursos
- Cacheo inteligente cuando aplique

### 5. Rendimiento Eficiente
- Queries optimizadas
- Procesamiento en paralelo cuando sea posible
- Lazy loading y streaming para grandes volÃºmenes
- Ãndices apropiados en bases de datos

### 6. Sostenibilidad y Escalabilidad
- Arquitectura modular y desacoplada
- ConfiguraciÃ³n externalizada
- DiseÃ±o para escalar horizontalmente
- Abstracciones reutilizables

### 7. Reusabilidad
- CÃ³digo DRY (Don't Repeat Yourself)
- Templates y funciones compartidas
- Convenciones claras y consistentes
- DocumentaciÃ³n de casos de uso

### 8. Principios de ProgramaciÃ³n
- SOLID principles
- KISS (Keep It Simple, Stupid)
- YAGNI (You Aren't Gonna Need It)
- Separation of Concerns
- Single Responsibility

### 9. Infraestructura como CÃ³digo
- Todo infrastructure debe ser cÃ³digo
- Versionado en Git
- Ambientes reproducibles
- Deployment automatizado

## Arquitectura Modular

El proyecto estÃ¡ diseÃ±ado para ser **completamente modular**. Puedes levantar componentes especÃ­ficos sin necesidad de correr todo el stack.

### Estructura de MÃ³dulos

```
snail-data-solutions/
â”œâ”€â”€ modules/                        # Todos los mÃ³dulos del proyecto
â”‚   â”œâ”€â”€ airflow-orchestration/     # MÃ³dulo de orquestaciÃ³n con Airflow + dbt
â”‚   â”‚   â”œâ”€â”€ dags/                  # DAGs de Airflow
â”‚   â”‚   â”‚   â”œâ”€â”€ setup_*            # DAGs de setup/inicializaciÃ³n
â”‚   â”‚   â”‚   â”œâ”€â”€ example_*          # DAGs de ejemplo/referencia
â”‚   â”‚   â”‚   â””â”€â”€ dbt_*              # DAGs que ejecutan dbt
â”‚   â”‚   â”œâ”€â”€ include/               # CÃ³digo compartido
â”‚   â”‚   â”‚   â”œâ”€â”€ dbt/               # Proyecto dbt
â”‚   â”‚   â”‚   â”œâ”€â”€ sql/               # SQL queries
â”‚   â”‚   â”‚   â””â”€â”€ config/            # Configuraciones YAML
â”‚   â”‚   â”œâ”€â”€ plugins/               # Plugins de Airflow
â”‚   â”‚   â”œâ”€â”€ tests/                 # Tests del mÃ³dulo
â”‚   â”‚   â”œâ”€â”€ Dockerfile             # Imagen de Astronomer
â”‚   â”‚   â”œâ”€â”€ requirements.txt       # Dependencias Python
â”‚   â”‚   â”œâ”€â”€ Makefile              # Comandos del mÃ³dulo
â”‚   â”‚   â””â”€â”€ README.md              # DocumentaciÃ³n del mÃ³dulo
â”‚   â”‚
â”‚   â””â”€â”€ aws-bedrock-agents/        # MÃ³dulo de agentes AI con AWS Bedrock
â”‚       â”œâ”€â”€ infrastructure/        # IaC con Terraform
â”‚       â”‚   â””â”€â”€ terraform/
â”‚       â”‚       â”œâ”€â”€ modules/       # MÃ³dulos reutilizables
â”‚       â”‚       â””â”€â”€ environments/  # dev/staging/prod
â”‚       â”œâ”€â”€ lambda-functions/      # CÃ³digo de Lambdas
â”‚       â”œâ”€â”€ step-functions/        # Workflows
â”‚       â”œâ”€â”€ tests/                 # Tests del mÃ³dulo
â”‚       â””â”€â”€ README.md              # DocumentaciÃ³n del mÃ³dulo
â”‚
â”œâ”€â”€ docs/                           # DocumentaciÃ³n general del proyecto
â”‚   â”œâ”€â”€ architecture/              # Diagramas y arquitectura
â”‚   â””â”€â”€ aws-bedrock-agents/        # Docs especÃ­ficos de Bedrock
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ ARCHITECTURE.md
â”‚       â””â”€â”€ COST_ANALYSIS.md
â”‚
â”œâ”€â”€ .claude/                        # ConfiguraciÃ³n de Claude Code
â”‚   â””â”€â”€ commands/
â”‚       â””â”€â”€ init.md
â”‚
â”œâ”€â”€ CLAUDE.md                       # Este archivo
â”œâ”€â”€ README.md                       # README principal
â””â”€â”€ .gitignore
```

### CÃ³mo Levantar Componentes EspecÃ­ficos

**MÃ³dulo Airflow Orchestration:**
```bash
# Desde el root del proyecto
cd modules/airflow-orchestration
astro dev start

# O usando make
make start

# Solo dbt (dentro del contenedor)
make dbt-run
```

**MÃ³dulo AWS Bedrock Agents:**
```bash
# Ver anÃ¡lisis de costos primero
cat docs/aws-bedrock-agents/COST_ANALYSIS.md

# Desplegar infraestructura (ambiente dev)
cd modules/aws-bedrock-agents/infrastructure/terraform/environments/dev
terraform init
terraform plan
terraform apply

# Ver documentaciÃ³n completa
cat modules/aws-bedrock-agents/README.md
```

**DAGs especÃ­ficos:**
Los DAGs se pueden activar/desactivar individualmente en la UI de Airflow o mediante tags.

## MÃ³dulos del Proyecto

### MÃ³dulo AWS Bedrock AI Agents

**DescripciÃ³n**: SoluciÃ³n modular para crear agentes de AI usando AWS Bedrock que procesan y responden consultas sobre diversos tipos de archivos.

**Componentes**:
- Amazon Bedrock (Claude/Titan) para modelos de lenguaje
- Knowledge Bases for Amazon Bedrock (RAG)
- AWS Lambda para procesamiento de documentos
- AWS Step Functions para orquestaciÃ³n de workflows
- Amazon S3 para almacenamiento (raw â†’ processed)
- Amazon Textract para OCR
- Terraform para IaC multi-ambiente

**Tipos de archivos soportados**:
- PDFs y documentos
- Datos estructurados (CSV, JSON)
- CÃ³digo fuente
- Multimedia (imÃ¡genes con texto vÃ­a OCR)

**Casos de uso**:
- AnÃ¡lisis de documentos y contratos
- Code assistant para bases de cÃ³digo
- Data analysis sobre datasets
- Document processing multi-fuente

**Arquitectura**:
- Pipeline de ingesta: S3 â†’ EventBridge â†’ Step Functions â†’ Lambda â†’ S3 processed â†’ Knowledge Base
- Agente AI: Bedrock Agent + Knowledge Base + Lambda custom actions
- Multi-ambiente: dev/staging/prod con Terraform

**DocumentaciÃ³n completa**:
- MÃ³dulo: `modules/aws-bedrock-agents/README.md`
- Arquitectura: `docs/aws-bedrock-agents/README.md`
- Costos: `docs/aws-bedrock-agents/COST_ANALYSIS.md`

**Costos estimados mensuales** (ver anÃ¡lisis completo en COST_ANALYSIS.md):
- MVP/Testing: **$10-30** (Claude Haiku + Pinecone free tier)
- ProducciÃ³n ligera: **$120-200** (Claude Sonnet + Aurora pgvector)
- ProducciÃ³n moderada: **$350-450** (Sonnet + OpenSearch 2 OCU)
- ProducciÃ³n intensiva: **$800-1,200** (Sonnet/Opus + OpenSearch escalado)

âš ï¸ **Nota**: El costo principal es el vector store (OpenSearch ~$175/mes mÃ­nimo). Para minimizar costos iniciales, usar alternativas como Aurora pgvector ($50-80/mes) o Pinecone free tier.

**Estado**: ğŸ”„ En desarrollo
- âœ… Arquitectura diseÃ±ada
- âœ… Estructura de directorios creada
- âœ… DocumentaciÃ³n base
- âœ… AnÃ¡lisis completo de costos
- âœ… Estructura modular documentada
- â³ MÃ³dulos de Terraform
- â³ Lambda functions
- â³ Step Functions workflows

## Convenciones del Proyecto

### Nomenclatura

**DAGs:**
- Prefijos: `setup_`, `example_`, `dbt_`, `etl_`, `ml_`
- Formato: `{prefix}_{descripcion_snake_case}.py`
- Ejemplos: `setup_sample_database.py`, `etl_customer_orders.py`

**Modelos dbt:**
- Staging: `stg_{source}_{entity}.sql` (ej: `stg_postgres_customers.sql`)
- Marts: `{tipo}_{descripcion}.sql` (ej: `fct_sales.sql`, `dim_customers.sql`)

**Archivos SQL:**
- Formato: `{numero}_{descripcion}.sql` si son secuenciales
- Formato: `{descripcion}.sql` si son independientes

**Variables de entorno:**
- MayÃºsculas con underscores: `DBT_HOST`, `AIRFLOW_CONN_POSTGRES`

**MÃ³dulos de Terraform:**
- Formato: `{servicio}-{proposito}` (ej: `bedrock-agent`, `lambda-processor`)
- Variables: snake_case (ej: `knowledge_base_name`, `lambda_timeout`)
- Outputs: snake_case con sufijo descriptivo (ej: `bucket_arn`, `lambda_function_name`)

**Lambda Functions:**
- Directorio: `{tipo}-{proposito}` (ej: `pdf-processor`, `query-handler`)
- Handler: `handler.py` con funciÃ³n `lambda_handler`
- Archivos: snake_case (ej: `pdf_extractor.py`, `text_processor.py`)

**Step Functions:**
- Archivos: `{workflow}-{proposito}.asl.json` (ej: `document-ingestion.asl.json`)
- Estados: PascalCase (ej: `ProcessDocument`, `IndexContent`)

### OrganizaciÃ³n de CÃ³digo

**SQL Externalizado:**
- NUNCA escribir SQL hardcoded en Python
- Todo SQL debe estar en `include/sql/` o modelos dbt
- Usar funciones helper para leer archivos SQL

**ConfiguraciÃ³n Externalizada:**
- ParÃ¡metros en archivos YAML en `include/config/`
- Variables de entorno en `.env` (nunca commiteadas)
- `.env.example` siempre actualizado

**Secrets y Credenciales:**
- NUNCA en cÃ³digo o configs commiteados
- Usar `.env` local (en `.gitignore`)
- Usar Airflow Connections/Variables en producciÃ³n
- Usar AWS Secrets Manager en cloud

### Tests

**Obligatorios para:**
- Todos los DAGs nuevos (test de import mÃ­nimo)
- Todos los modelos dbt (unique, not_null como mÃ­nimo)
- Funciones compartidas/helpers

**UbicaciÃ³n:**
- Tests de DAGs: `tests/dags/`
- Tests de dbt: `include/dbt/models/*/schema.yml`
- Tests de helpers: `tests/unit/`

### DocumentaciÃ³n

**README.md:**
- InformaciÃ³n para usuarios/developers
- Quick start y comandos bÃ¡sicos
- Troubleshooting comÃºn

**CLAUDE.md (este archivo):**
- Contexto del proyecto para Claude
- Principios y valores
- Instrucciones especÃ­ficas para desarrollo
- **DEBE actualizarse en cada cambio significativo**

**Docstrings:**
- Todas las funciones pÃºblicas
- Todos los DAGs (en el docstring del DAG)
- Modelos dbt (en archivos `.yml`)

## Instrucciones para Claude

### Al Trabajar en Este Proyecto

1. **SIEMPRE lee el cÃ³digo existente antes de modificar**
   - Entiende los patrones actuales
   - MantÃ©n consistencia con el estilo existente

2. **Sigue los principios del proyecto**
   - Revisa la secciÃ³n "Principios y Valores" antes de proponer cambios
   - Optimiza para reusabilidad y escalabilidad

3. **MantÃ©n la modularidad**
   - Cada DAG debe ser independiente
   - Cada mÃ³dulo de dbt debe ser autocontenido
   - Usa configuraciones externalizadas

4. **Documenta todos los cambios**
   - Actualiza README.md si afecta uso/comandos
   - Actualiza CLAUDE.md si cambian principios/estructura
   - Agrega docstrings a cÃ³digo nuevo
   - Actualiza archivos `.yml` de dbt

5. **Tests son obligatorios**
   - Agrega tests para cÃ³digo nuevo
   - Verifica que tests existentes pasen
   - No hacer commits si los tests fallan

6. **Seguridad primero**
   - Nunca expongas credenciales
   - Valida inputs externos
   - Usa conexiones de Airflow para DBs

### Al Crear Nuevas Soluciones

1. **Identifica si es reusable**
   - Si sÃ­: crear template genÃ©rico en `/templates` (futuro)
   - Si no: documentar caso de uso especÃ­fico

2. **Sigue nomenclatura establecida**
   - Usa prefijos apropiados
   - Nombres descriptivos y claros

3. **Externaliza configuraciÃ³n**
   - ParÃ¡metros en YAML
   - Credenciales en variables de entorno
   - Documentar configuraciÃ³n en README

4. **Agrega ejemplos y tests**
   - Al menos un ejemplo de uso
   - Tests bÃ¡sicos de funcionamiento

### Al Actualizar DocumentaciÃ³n

**Actualiza CLAUDE.md cuando:**
- Se agreguen nuevos mÃ³dulos/componentes
- Cambien principios o convenciones
- Se agreguen nuevas herramientas al stack
- Se modifique la estructura del proyecto

**Actualiza README.md cuando:**
- Cambien comandos o instrucciones de uso
- Se agreguen nuevas dependencias
- Cambien los pasos de setup
- Se agreguen nuevos DAGs relevantes

## Estado Actual del Proyecto

### Implementado
- âœ… Setup de Airflow con Astronomer
- âœ… IntegraciÃ³n de dbt con PostgreSQL
- âœ… Base de datos de ejemplo (e-commerce)
- âœ… DAGs de ejemplo (ETL, CRUD, branching)
- âœ… Estructura modular con carpeta `modules/`
- âœ… SQL y configs externalizados
- âœ… Tests bÃ¡sicos de DAGs
- âœ… Tests de dbt con validaciones
- âœ… Makefile con comandos Ãºtiles
- âœ… DocumentaciÃ³n en README.md
- âœ… DocumentaciÃ³n CLAUDE.md con principios y convenciones
- âœ… Comando `/init` para cargar contexto automÃ¡ticamente
- âœ… AWS CLI configurado y verificado

### En Progreso
- ğŸ”„ MÃ³dulo AWS Bedrock AI Agents
  - âœ… Arquitectura diseÃ±ada con diagrama de flujo
  - âœ… Estructura modular documentada (modules/aws-bedrock-agents/)
  - âœ… DocumentaciÃ³n completa del mÃ³dulo
  - âœ… AnÃ¡lisis detallado de costos (MVP: $10-30/mes, Prod: $120-1,200/mes)
  - âœ… Estrategias de optimizaciÃ³n de costos identificadas
  - âœ… Alternativas de vector store evaluadas (OpenSearch vs Aurora vs Pinecone)
  - â³ MÃ³dulos de Terraform (bedrock, lambda, step-functions, s3, iam)
  - â³ Lambda functions para procesamiento de documentos
  - â³ Step Functions workflows

### Por Implementar
- â³ Templates reutilizables para DAGs comunes
- â³ CI/CD pipeline (GitHub Actions)
- â³ IntegraciÃ³n Airflow + AWS (S3, Redshift operators)
- â³ Databricks integration
- â³ Deployment a Astronomer Cloud
- â³ Monitoreo y alertas
- â³ CatÃ¡logo de datos (dbt docs)

## Comandos de Claude Code

### Iniciar una Nueva SesiÃ³n

Cuando abras una nueva ventana/sesiÃ³n de Claude Code, ejecuta:

```
/init
```

Este comando carga automÃ¡ticamente todo el contexto del proyecto desde CLAUDE.md, incluyendo principios, convenciones, y arquitectura.

### Otros Comandos Ãštiles

```
/context        # Ver quÃ© contexto estÃ¡ cargado actualmente
/help           # Ver todos los comandos disponibles
```

## Comandos del Proyecto

```bash
# Desarrollo
make start              # Levantar todo el stack
make stop               # Detener todo
make restart            # Reiniciar
make logs               # Ver logs

# dbt
make dbt-debug          # Verificar config dbt
make dbt-run            # Ejecutar modelos
make dbt-test           # Ejecutar tests dbt

# Testing
make pytest             # Tests de Airflow

# Limpieza
make clean              # Limpiar todo y empezar fresh
```

## Recursos

- [DocumentaciÃ³n Airflow](https://airflow.apache.org/docs/)
- [DocumentaciÃ³n dbt](https://docs.getdbt.com/)
- [Astronomer Docs](https://www.astronomer.io/docs/)
- [AWS Best Practices](https://aws.amazon.com/architecture/well-architected/)

---

**Ãšltima actualizaciÃ³n**: 2025-11-23
**Mantenedor**: Snail Data Solutions
**VersiÃ³n**: 1.0.0
