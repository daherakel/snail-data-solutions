# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

---

# Instrucciones del Proyecto Snail Data Solutions

## Sobre Snail Data Solutions

**Snail Data Solutions** es una consultora especializada en Data Engineering y AI. Este repositorio contiene soluciones, proyectos, templates y playgrounds reutilizables para acelerar implementaciones de clientes y servir como base de conocimiento.

## Quick Reference

### Common Commands (Airflow Module)
```bash
# Working directory
cd modules/airflow-orchestration

# Core operations
make start          # Start Airflow (http://localhost:8080, admin/admin)
make stop           # Stop Airflow
make logs           # View all logs
make shell          # Open shell in container

# dbt operations
make dbt-run        # Run dbt models
make dbt-test       # Run dbt tests
make dbt-debug      # Verify dbt configuration

# Testing
make pytest         # Run Airflow tests

# Single test
astro dev pytest tests/dags/test_specific_dag.py
```

### Architecture at a Glance
- **Airflow Orchestration Module**: `modules/airflow-orchestration/`
  - DAGs: `dags/` (setup_*, example_*, dbt_*)
  - dbt models: `include/dbt/models/` (staging/, marts/)
  - SQL queries: `include/sql/`
  - Config: `include/config/`

- **Snail Doc Module**: `modules/snail-doc/` (AI Document Assistant)
  - Frontend: `frontend/` (Next.js UI)
  - Infrastructure: `infrastructure/terraform/`
  - Lambda functions: `lambda-functions/`

### Key Files to Read First
- This file (CLAUDE.md) for project context
- `modules/snail-doc/README.md` for Snail Doc module
- `modules/airflow-orchestration/README.md` for Airflow setup
- `docs/COST_AND_SCALING.md` for AWS cost estimates

## Objetivo del Repositorio

Este repositorio funciona como:
- **Biblioteca de soluciones**: Templates y patrones probados listos para usar
- **Playground**: Espacio para experimentar con nuevas tecnolog√≠as y patrones
- **Base de conocimiento**: Ejemplos y documentaci√≥n de mejores pr√°cticas
- **Acelerador de proyectos**: C√≥digo reutilizable para implementaciones de clientes

## Stack Tecnol√≥gico

### Actual
- **Orquestaci√≥n**: Apache Airflow 2.10.3 (Astro Runtime 12.5.0)
- **Transformaci√≥n**: dbt 1.10.15 con adaptador PostgreSQL
- **Plataforma**: Astronomer (desarrollo local y deployment)
- **Cloud**: AWS (Bedrock, Lambda, Step Functions, S3, Textract)
- **Base de Datos**: PostgreSQL 13 (para ejemplos locales)
- **Contenedores**: Docker
- **IaC**: Terraform (multi-ambiente: dev/staging/prod)

### Pr√≥ximamente
- **Databricks**: Para procesamiento de big data y ML

## Principios y Valores del Proyecto

Todo c√≥digo y arquitectura debe seguir estos principios:

### 1. Excelencia Operativa
- C√≥digo limpio, legible y bien documentado
- Automatizaci√≥n de procesos repetitivos
- Monitoreo y observabilidad desde el dise√±o
- Documentaci√≥n siempre actualizada

### 2. Seguridad
- Credenciales NUNCA en c√≥digo (usar variables de entorno, secrets managers)
- Principio de privilegios m√≠nimos
- Validaci√≥n de inputs y outputs
- Logs sin informaci√≥n sensible

### 3. Confiabilidad y Fiabilidad
- Tests unitarios y de integraci√≥n
- Manejo de errores robusto
- Idempotencia en todas las operaciones
- Retry logic con backoff exponencial

### 4. Optimizaci√≥n de Costos
- Recursos dimensionados apropiadamente
- Limpieza de recursos temporales
- Monitoreo de uso de recursos
- Cacheo inteligente cuando aplique

### 5. Rendimiento Eficiente
- Queries optimizadas
- Procesamiento en paralelo cuando sea posible
- Lazy loading y streaming para grandes vol√∫menes
- √çndices apropiados en bases de datos

### 6. Sostenibilidad y Escalabilidad
- Arquitectura modular y desacoplada
- Configuraci√≥n externalizada
- Dise√±o para escalar horizontalmente
- Abstracciones reutilizables

### 7. Reusabilidad
- C√≥digo DRY (Don't Repeat Yourself)
- Templates y funciones compartidas
- Convenciones claras y consistentes
- Documentaci√≥n de casos de uso

### 8. Principios de Programaci√≥n
- SOLID principles
- KISS (Keep It Simple, Stupid)
- YAGNI (You Aren't Gonna Need It)
- Separation of Concerns
- Single Responsibility

### 9. Infraestructura como C√≥digo
- Todo infrastructure debe ser c√≥digo
- Versionado en Git
- Ambientes reproducibles
- Deployment automatizado

## Arquitectura Modular

El proyecto est√° dise√±ado para ser **completamente modular**. Puedes levantar componentes espec√≠ficos sin necesidad de correr todo el stack.

### Estructura de M√≥dulos

```
snail-data-solutions/
‚îú‚îÄ‚îÄ modules/                           # M√≥dulos SaaS independientes
‚îÇ   ‚îú‚îÄ‚îÄ snail-doc/                    # üêå Asistente AI de documentos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frontend/                 # Next.js UI (chat, upload, analytics)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/           # IaC con Terraform
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ terraform/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ modules/          # M√≥dulos reutilizables
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ environments/     # dev/staging/prod
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lambda-functions/         # AWS Lambda functions
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf-processor/        # Procesa PDFs ‚Üí embeddings
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query-handler/        # RAG queries
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ slack-handler/        # Integraci√≥n Slack
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shared/                   # C√≥digo compartido
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/                  # Scripts de deployment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ airflow-orchestration/        # ‚öôÔ∏è Data pipelines
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dags/                     # DAGs de Airflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ include/                  # dbt, SQL, config
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ contact-lambda/               # üìß Formulario de contacto
‚îÇ
‚îú‚îÄ‚îÄ docs/                              # Documentaci√≥n general
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md                 # Gu√≠a de deployment
‚îÇ   ‚îú‚îÄ‚îÄ COST_AND_SCALING.md          # Costos y escalamiento
‚îÇ   ‚îî‚îÄ‚îÄ archive/                      # Docs hist√≥ricos
‚îÇ
‚îú‚îÄ‚îÄ CLAUDE.md                          # Este archivo
‚îî‚îÄ‚îÄ README.md                          # README principal
```

### C√≥mo Levantar Componentes Espec√≠ficos

**M√≥dulo Airflow Orchestration:**
```bash
# Desde el root del proyecto
cd modules/airflow-orchestration
astro dev start

# O usando make
make start

# Solo dbt (dentro del contenedor)
make dbt-run
```

**M√≥dulo Snail Doc (AI Document Assistant):**
```bash
# Ver an√°lisis de costos primero
cat docs/COST_AND_SCALING.md

# Desplegar infraestructura (ambiente dev)
cd modules/snail-doc/infrastructure/terraform/environments/dev
terraform init
terraform plan
terraform apply

# Iniciar frontend
cd modules/snail-doc/frontend
npm install && npm run dev

# Ver documentaci√≥n completa
cat modules/snail-doc/README.md
```

**DAGs espec√≠ficos:**
Los DAGs se pueden activar/desactivar individualmente en la UI de Airflow o mediante tags.

## M√≥dulos del Proyecto

### üêå Snail Doc - AI Document Assistant

**Descripci√≥n**: Asistente inteligente de documentos usando AWS Bedrock con RAG. Procesa PDFs y responde consultas con contexto.

**Componentes**:
- Amazon Bedrock (Claude/Titan) para modelos de lenguaje
- Knowledge Bases for Amazon Bedrock (RAG)
- AWS Lambda para procesamiento de documentos
- AWS Step Functions para orquestaci√≥n de workflows
- Amazon S3 para almacenamiento (raw ‚Üí processed)
- Amazon Textract para OCR
- Terraform para IaC multi-ambiente

**Tipos de archivos soportados**:
- PDFs y documentos
- Datos estructurados (CSV, JSON)
- C√≥digo fuente
- Multimedia (im√°genes con texto v√≠a OCR)

**Casos de uso**:
- An√°lisis de documentos y contratos
- Code assistant para bases de c√≥digo
- Data analysis sobre datasets
- Document processing multi-fuente

**Arquitectura**:
- Pipeline de ingesta: S3 ‚Üí EventBridge ‚Üí Step Functions ‚Üí Lambda ‚Üí S3 processed ‚Üí Knowledge Base
- Agente AI: Bedrock Agent + Knowledge Base + Lambda custom actions
- Multi-ambiente: dev/staging/prod con Terraform

**Documentaci√≥n completa**:
- **[Module README](modules/snail-doc/README.md)** - Features & quick start
- **[DEPLOYMENT.md](docs/DEPLOYMENT.md)** - Deployment guide (dev/staging/prod)
- **[COST_AND_SCALING.md](docs/COST_AND_SCALING.md)** - Complete cost analysis
- **[Frontend](modules/snail-doc/frontend/README.md)** - Next.js chat interface

**Costos estimados mensuales** (ver [COST_AND_SCALING.md](docs/COST_AND_SCALING.md)):
- **POC/Development**: $0.78-$3 (FAISS + Titan Express)
- **Production Light**: $15-$30 (FAISS + Llama 3.3 70B, 500 queries/month)
- **Production Moderate**: $120-$200 (FAISS + Claude Sonnet, 5K queries/month)
- **Production Intensive**: $450-$800 (FAISS or Aurora pgvector + Claude, 20K+ queries/month)

‚úÖ **Key advantage**: Using FAISS instead of OpenSearch reduces base cost from $175/month to ~$0.00/month (225x cheaper).

**Estado**: ‚úÖ **Production-Ready (Nov 2025)**
- ‚úÖ Infrastructure deployed with Terraform (dev environment)
- ‚úÖ FAISS vector store implemented (Lambda layer)
- ‚úÖ Lambda functions: pdf-processor, query-handler
- ‚úÖ DynamoDB: query cache, rate limiting, conversations
- ‚úÖ EventBridge triggers for automatic PDF processing
- ‚úÖ Frontend: Next.js with chat UI, conversation management
- ‚úÖ End-to-end tested and documented

## Convenciones del Proyecto

### Nomenclatura

**DAGs:**
- Prefijos: `setup_`, `example_`, `dbt_`, `etl_`, `ml_`
- Formato: `{prefix}_{descripcion_snake_case}.py`
- Ejemplos: `setup_sample_database.py`, `etl_customer_orders.py`

**Modelos dbt:**
- Staging: `stg_{source}_{entity}.sql` (ej: `stg_postgres_customers.sql`)
- Marts: `{tipo}_{descripcion}.sql` (ej: `fct_sales.sql`, `dim_customers.sql`)

**Archivos SQL:**
- Formato: `{numero}_{descripcion}.sql` si son secuenciales
- Formato: `{descripcion}.sql` si son independientes

**Variables de entorno:**
- May√∫sculas con underscores: `DBT_HOST`, `AIRFLOW_CONN_POSTGRES`

**M√≥dulos de Terraform:**
- Formato: `{servicio}-{proposito}` (ej: `bedrock-agent`, `lambda-processor`)
- Variables: snake_case (ej: `knowledge_base_name`, `lambda_timeout`)
- Outputs: snake_case con sufijo descriptivo (ej: `bucket_arn`, `lambda_function_name`)

**Lambda Functions:**
- Directorio: `{tipo}-{proposito}` (ej: `pdf-processor`, `query-handler`)
- Handler: `handler.py` con funci√≥n `lambda_handler`
- Archivos: snake_case (ej: `pdf_extractor.py`, `text_processor.py`)

**Step Functions:**
- Archivos: `{workflow}-{proposito}.asl.json` (ej: `document-ingestion.asl.json`)
- Estados: PascalCase (ej: `ProcessDocument`, `IndexContent`)

### Organizaci√≥n de C√≥digo

**SQL Externalizado:**
- NUNCA escribir SQL hardcoded en Python
- Todo SQL debe estar en `include/sql/` o modelos dbt
- Usar funciones helper para leer archivos SQL

**Configuraci√≥n Externalizada:**
- Par√°metros en archivos YAML en `include/config/`
- Variables de entorno en `.env` (nunca commiteadas)
- `.env.example` siempre actualizado

**Secrets y Credenciales:**
- NUNCA en c√≥digo o configs commiteados
- Usar `.env` local (en `.gitignore`)
- Usar Airflow Connections/Variables en producci√≥n
- Usar AWS Secrets Manager en cloud

### Tests

**Obligatorios para:**
- Todos los DAGs nuevos (test de import m√≠nimo)
- Todos los modelos dbt (unique, not_null como m√≠nimo)
- Funciones compartidas/helpers

**Ubicaci√≥n:**
- Tests de DAGs: `tests/dags/`
- Tests de dbt: `include/dbt/models/*/schema.yml`
- Tests de helpers: `tests/unit/`

### Documentaci√≥n

**README.md:**
- Informaci√≥n para usuarios/developers
- Quick start y comandos b√°sicos
- Troubleshooting com√∫n

**CLAUDE.md (este archivo):**
- Contexto del proyecto para Claude
- Principios y valores
- Instrucciones espec√≠ficas para desarrollo
- **DEBE actualizarse en cada cambio significativo**

**Docstrings:**
- Todas las funciones p√∫blicas
- Todos los DAGs (en el docstring del DAG)
- Modelos dbt (en archivos `.yml`)

## Instrucciones para Claude

### Typical Development Workflows

#### Adding a New DAG
1. Read existing DAGs in `modules/airflow-orchestration/dags/` to understand patterns
2. Create new DAG file with appropriate prefix (setup_*, example_*, etl_*, etc.)
3. Externalize SQL queries to `include/sql/`
4. Externalize configuration to `include/config/` (YAML)
5. Add test in `tests/dags/test_your_dag.py`
6. Run: `cd modules/airflow-orchestration && make start`
7. Verify DAG appears in UI at http://localhost:8080
8. Run tests: `make pytest`

#### Adding a dbt Model
1. Navigate to `modules/airflow-orchestration/include/dbt/models/`
2. Create staging model in `staging/stg_{source}_{entity}.sql`
3. Create mart model in `marts/{fct|dim}_{description}.sql`
4. Add tests in corresponding `schema.yml` file (unique, not_null minimum)
5. Run: `make dbt-run` to materialize
6. Run: `make dbt-test` to validate
7. Update documentation in schema.yml

#### Modifying Snail Doc Infrastructure
1. Read existing Terraform modules in `modules/snail-doc/infrastructure/terraform/modules/`
2. Review cost analysis first: `docs/COST_AND_SCALING.md`
3. Modify Terraform module or create new one
4. Update variables in `environments/dev/variables.tf`
5. Plan: `terraform plan` from environment directory
6. Apply: `terraform apply` (with user confirmation)
7. Document changes in module README and update costs if applicable

#### Running Single Test
```bash
cd modules/airflow-orchestration
astro dev pytest tests/dags/test_specific_dag.py::test_function_name -v
```

#### Debugging a DAG Issue
1. Check scheduler logs: `make logs-scheduler`
2. Verify DAG syntax: `astro dev bash -c "python dags/your_dag.py"`
3. Check Airflow UI for import errors
4. Verify SQL files exist in `include/sql/`
5. Check database connection: `make dbt-debug`

### Code Patterns to Follow

**SQL Externalization Pattern:**
```python
# Read SQL from file (see example DAGs)
def read_sql_file(filepath):
    with open(filepath, 'r') as f:
        return f.read()

# Usage in DAG
sql = read_sql_file('include/sql/analytics/query.sql')
```

**YAML Configuration Pattern:**
```python
# Load config from YAML (see example DAGs)
import yaml

with open('include/config/dag_config.yaml') as f:
    config = yaml.safe_load(f)

# Access config values
schedule = config['dag']['schedule']
```

**dbt Model Pattern:**
```sql
-- Staging: stg_source_entity.sql (materialized as view)
-- Clean and standardize raw data
with source as (
    select * from {{ source('postgres', 'raw_table') }}
)
select
    id,
    lower(trim(name)) as name_clean,
    created_at
from source

-- Marts: fct_entity.sql (materialized as table)
-- Business logic and aggregations
select
    d.id,
    d.name_clean,
    count(*) as total_count
from {{ ref('stg_source_entity') }} d
group by 1, 2
```

**Airflow Connection Pattern:**
```python
# Use Airflow connections, not hardcoded credentials
from airflow.providers.postgres.hooks.postgres import PostgresHook

pg_hook = PostgresHook(postgres_conn_id='postgres_default')
```

### Al Trabajar en Este Proyecto

1. **SIEMPRE lee el c√≥digo existente antes de modificar**
   - Entiende los patrones actuales mostrados arriba
   - Mant√©n consistencia con el estilo existente
   - Busca ejemplos similares en `dags/example_*.py`

2. **Sigue los principios del proyecto**
   - Revisa la secci√≥n "Principios y Valores" antes de proponer cambios
   - Optimiza para reusabilidad y escalabilidad
   - Aplica los patrones de c√≥digo establecidos

3. **Mant√©n la modularidad**
   - Cada DAG debe ser independiente
   - Cada m√≥dulo de dbt debe ser autocontenido
   - Usa configuraciones externalizadas

4. **Documenta todos los cambios**
   - Actualiza README.md si afecta uso/comandos
   - Actualiza CLAUDE.md si cambian principios/estructura
   - Agrega docstrings a c√≥digo nuevo
   - Actualiza archivos `.yml` de dbt

5. **Tests son obligatorios**
   - Agrega tests para c√≥digo nuevo
   - Verifica que tests existentes pasen
   - No hacer commits si los tests fallan

6. **Seguridad primero**
   - Nunca expongas credenciales
   - Valida inputs externos
   - Usa conexiones de Airflow para DBs

### Al Crear Nuevas Soluciones

1. **Identifica si es reusable**
   - Si s√≠: crear template gen√©rico en `/templates` (futuro)
   - Si no: documentar caso de uso espec√≠fico

2. **Sigue nomenclatura establecida**
   - Usa prefijos apropiados
   - Nombres descriptivos y claros

3. **Externaliza configuraci√≥n**
   - Par√°metros en YAML
   - Credenciales en variables de entorno
   - Documentar configuraci√≥n en README

4. **Agrega ejemplos y tests**
   - Al menos un ejemplo de uso
   - Tests b√°sicos de funcionamiento

### Al Actualizar Documentaci√≥n

**Actualiza CLAUDE.md cuando:**
- Se agreguen nuevos m√≥dulos/componentes
- Cambien principios o convenciones
- Se agreguen nuevas herramientas al stack
- Se modifique la estructura del proyecto

**Actualiza README.md cuando:**
- Cambien comandos o instrucciones de uso
- Se agreguen nuevas dependencias
- Cambien los pasos de setup
- Se agreguen nuevos DAGs relevantes

### Common Issues and Solutions

**Issue: DAG not appearing in Airflow UI**
```bash
# Check scheduler logs for import errors
make logs-scheduler

# Verify DAG syntax
astro dev bash -c "python dags/your_dag.py"

# Common causes:
# - Syntax errors in DAG file
# - Missing dependencies in requirements.txt
# - Import errors (missing modules)
```

**Issue: dbt models failing**
```bash
# Verify dbt connection
make dbt-debug

# Check compiled SQL
make dbt-compile

# Common causes:
# - Database connection issues (check .env)
# - Missing source tables (run setup_sample_database DAG first)
# - Invalid Jinja syntax in models
# - Missing dependencies in dbt_project.yml
```

**Issue: "Permission denied" or database connection errors**
```bash
# Verify environment variables
astro dev bash -c "env | grep DBT"

# Restart Airflow to reload .env
make restart

# Common causes:
# - .env file not loaded
# - PostgreSQL container not running
# - Incorrect credentials in .env
```

**Issue: Changes not reflected after editing code**
```bash
# For DAG changes: Wait ~30 seconds (auto-reload)
# For include/ changes: Restart required
make restart

# For dbt changes: Recompile
make dbt-compile
```

**Issue: Docker resource issues**
```bash
# Clean up Docker resources
make clean

# Remove all volumes and start fresh
docker system prune -a --volumes

# Restart from scratch
make start
```

## Estado Actual del Proyecto

### Implementado
- ‚úÖ Setup de Airflow con Astronomer
- ‚úÖ Integraci√≥n de dbt con PostgreSQL
- ‚úÖ Base de datos de ejemplo (e-commerce)
- ‚úÖ DAGs de ejemplo (ETL, CRUD, branching)
- ‚úÖ Estructura modular con carpeta `modules/`
- ‚úÖ SQL y configs externalizados
- ‚úÖ Tests b√°sicos de DAGs
- ‚úÖ Tests de dbt con validaciones
- ‚úÖ Makefile con comandos √∫tiles
- ‚úÖ Documentaci√≥n en README.md
- ‚úÖ Documentaci√≥n CLAUDE.md con principios y convenciones
- ‚úÖ Comando `/init` para cargar contexto autom√°ticamente
- ‚úÖ AWS CLI configurado y verificado

### Implementado Recientemente (Nov 2025)
- ‚úÖ **M√≥dulo Snail Doc - AI Document Assistant - COMPLETAMENTE DESPLEGADO**
  - ‚úÖ Arquitectura dise√±ada con diagrama de flujo
  - ‚úÖ Estructura modular documentada (modules/snail-doc/)
  - ‚úÖ Documentaci√≥n completa del m√≥dulo
  - ‚úÖ An√°lisis detallado de costos (MVP: $10-30/mes, Prod: $120-1,200/mes)
  - ‚úÖ Estrategias de optimizaci√≥n de costos identificadas
  - ‚úÖ Alternativas de vector store evaluadas (FAISS seleccionado)
  - ‚úÖ **Infraestructura Terraform desplegada en AWS (ambiente dev)**
    - S3 buckets (raw, processed, faiss-backup)
    - DynamoDB tables (query-cache, rate-limiting)
    - Lambda functions (pdf-processor, query-handler) con FAISS layer
    - Step Functions state machine para orquestaci√≥n
    - EventBridge rules para triggers autom√°ticos
    - IAM roles con permisos correctos
  - ‚úÖ **Lambda PDF Processor funcionando** - Procesa PDFs y genera embeddings FAISS
  - ‚úÖ **Lambda Query Handler funcionando** - Responde consultas con RAG usando Bedrock Claude
  - ‚úÖ **Cache de queries en DynamoDB** - Optimizaci√≥n de costos funcionando
  - ‚úÖ **Frontend Next.js desplegado localmente** - UI moderna con chat, upload y analytics
  - ‚úÖ **Sistema end-to-end probado** - PDF ‚Üí FAISS ‚Üí Query ‚Üí Respuesta con cache
  - üîÑ Slack Handler (c√≥digo listo, requiere credenciales de Slack para deploy)

### Por Implementar
- ‚è≥ Templates reutilizables para DAGs comunes
- ‚è≥ CI/CD pipeline (GitHub Actions)
- ‚è≥ Integraci√≥n Airflow + AWS (S3, Redshift operators)
- ‚è≥ Databricks integration
- ‚è≥ Deployment a Astronomer Cloud
- ‚è≥ Monitoreo y alertas
- ‚è≥ Cat√°logo de datos (dbt docs)

## Documentation Quick Reference

### Getting Started
- **[README.md](README.md)** - Project overview and quick links
- **[DEPLOYMENT.md](docs/DEPLOYMENT.md)** - Complete deployment guide for all environments
- **[COST_AND_SCALING.md](docs/COST_AND_SCALING.md)** - Cost analysis and scaling strategies

### Module Documentation
- **[Snail Doc](modules/snail-doc/README.md)** - AI Document Assistant (architecture, features, quick start)
- **[Airflow Orchestration](modules/airflow-orchestration/README.md)** - Data pipeline module
- **[Snail Doc Frontend](modules/snail-doc/frontend/README.md)** - Next.js chat UI documentation

### Reference Documentation
- **[Archived Evaluations](docs/archive/)** - Historical comparisons (vector DBs, ChromaDB POC)
- **[Terraform Dev](modules/snail-doc/infrastructure/terraform/environments/dev/README.md)** - Dev environment setup

### Quick Navigation
| Need to... | Go to... |
|-----------|----------|
| Deploy the system | [DEPLOYMENT.md](docs/DEPLOYMENT.md) |
| Understand costs | [COST_AND_SCALING.md](docs/COST_AND_SCALING.md) |
| Configure frontend | [modules/snail-doc/frontend/README.md](modules/snail-doc/frontend/README.md) |
| Modify Lambda code | `modules/snail-doc/lambda-functions/` |
| Change infrastructure | `modules/snail-doc/infrastructure/terraform/` |
| View archived docs | [docs/archive/](docs/archive/) |

---

## Comandos de Claude Code

### Iniciar una Nueva Sesi√≥n

Cuando abras una nueva ventana/sesi√≥n de Claude Code, ejecuta:

```
/init
```

Este comando carga autom√°ticamente todo el contexto del proyecto desde CLAUDE.md, incluyendo principios, convenciones, y arquitectura.

### Otros Comandos √ötiles

```
/context        # Ver qu√© contexto est√° cargado actualmente
/help           # Ver todos los comandos disponibles
```

## Comandos del Proyecto

```bash
# Desarrollo
make start              # Levantar todo el stack
make stop               # Detener todo
make restart            # Reiniciar
make logs               # Ver logs

# dbt
make dbt-debug          # Verificar config dbt
make dbt-run            # Ejecutar modelos
make dbt-test           # Ejecutar tests dbt

# Testing
make pytest             # Tests de Airflow

# Limpieza
make clean              # Limpiar todo y empezar fresh
```

## Recursos

- [Documentaci√≥n Airflow](https://airflow.apache.org/docs/)
- [Documentaci√≥n dbt](https://docs.getdbt.com/)
- [Astronomer Docs](https://www.astronomer.io/docs/)
- [AWS Best Practices](https://aws.amazon.com/architecture/well-architected/)

---

**√öltima actualizaci√≥n**: 2025-11-27
**Mantenedor**: Snail Data Solutions
**Versi√≥n**: 2.0.0 (Reorganizaci√≥n modular - Snail Doc)
